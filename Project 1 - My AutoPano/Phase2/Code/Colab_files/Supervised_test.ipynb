{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOx1UrwPCRUw",
        "outputId": "a60f059a-508c-4992-e97a-612c42062f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWgylrRbyR8c",
        "outputId": "1c326787-b7cb-4a0b-edd9-28d98b2b224b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh6PvmyQrBL9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import random\n",
        "from skimage import data, exposure, img_as_float\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import shutil\n",
        "import string\n",
        "import pandas as pd\n",
        "import math as m\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import glob\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getCornersFromH4pt(corner1, H4pt):\n",
        "    corners1 = np.array(corner1.copy())\n",
        "    del_corners = H4pt.reshape(2,4).T\n",
        "    corners2 = corners1 + del_corners\n",
        "    return corners2\n",
        "\n",
        "def drawCorners(image, corners, color):\n",
        "\n",
        "    corners_ = np.array(corners.copy())\n",
        "    r = corners_[2,:].copy()\n",
        "    corners_[2,:] = corners_[3,:]\n",
        "    corners_[3,:] = r\n",
        "    corners_ = corners_.reshape(-1,1,2)\n",
        "#     print(corners_)\n",
        "    corners_ = corners_.astype(int)\n",
        "    image_corners = cv2.polylines(image.copy(),[corners_],True,color, 4)\n",
        "    return image_corners\n",
        "\n",
        "def getHfromH4pt(corners1, H4pt):\n",
        "#     print(\"H4pt is: \")\n",
        "#     print(H4pt.reshape(2,4).T)\n",
        "\n",
        "    del_corners = H4pt.reshape(2,4).T\n",
        "    \n",
        "    corners1 = np.array(corners1)\n",
        "#     print(\"corner1 is: \")\n",
        "#     print(corners1)\n",
        "\n",
        "    corners2 = corners1 + del_corners\n",
        "#     print(\"corner2 is: \")\n",
        "#     print(corners2)\n",
        "\n",
        "    H = cv2.getPerspectiveTransform(np.float32(corners1), np.float32(corners2))\n",
        "#     print(\"H is:\")\n",
        "#     print(H)\n",
        "    return H\n",
        "\n",
        "def warpImage(img, corners, H):\n",
        "    image = img.copy()\n",
        "    h, w, _= image.shape\n",
        "\n",
        "    corners_ = np.array(corners)\n",
        "    corners_ = corners_.reshape((-1,1,2))\n",
        "\n",
        "    image_transformed = cv2.warpPerspective(image, H, (w,h))\n",
        "    corner_transformed = cv2.perspectiveTransform(np.float32(corners_), H)\n",
        "    corner_transformed = corner_transformed.astype(int)\n",
        "    \n",
        "    return image_transformed, corner_transformed"
      ],
      "metadata": {
        "id": "Izdez8vFsNQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Visualise_supervised(i, BasePath, SavePath, train_labels, pointsList, image):\n",
        "    \n",
        "    Y_Pred = np.load(SavePath+'/h4pt_Pred.npy')\n",
        "    print(Y_Pred.shape)\n",
        "\n",
        "\n",
        "    corners_a = pointsList[i,:,:,0]\n",
        "    corners_b = pointsList[i,:,:,1]\n",
        "\n",
        "    imA = cv2.imread(image)\n",
        "\n",
        "    H_AB = getHfromH4pt(corners_a, train_labels[i])\n",
        "    imB, corners_b_cal = warpImage(imA, corners_a, H_AB)\n",
        "\n",
        "    imA_corners = drawCorners(imA, corners_a, (0,0,255))\n",
        "    imB_corners = drawCorners(imB, corners_b_cal, (0,0,255))\n",
        "\n",
        "    mae = mean_absolute_error(Y_Pred[i], train_labels[i])\n",
        "    print(\"Mean absolute Error for image at index \",i, \":  \",mae)\n",
        "\n",
        "    corners_b_pred = getCornersFromH4pt(corners_a, Y_Pred[i])\n",
        "    # imA_corners = drawCorners(imA, pts1, (0,0,255))\n",
        "    imB_corners_pred = drawCorners(imB_corners, corners_b_pred, (0,255,255))\n",
        "    imB_corners_pred = cv2.putText(imB_corners_pred, \"MCE: \"+str(round(mae,3)),(150,230),cv2.FONT_HERSHEY_SIMPLEX,0.75,(255,0,0),2,cv2.LINE_AA)\n",
        "    return np.hstack((imA_corners,imB_corners_pred))"
      ],
      "metadata": {
        "id": "dBSN73DEsEku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def homography_model(img, image_size, batch_size):\n",
        "  x = tf.layers.conv2d(inputs=img, name='Conv2D1', padding='same',filters=64, kernel_size=[3,3], activation=None)\n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm1')\n",
        "  x = tf.nn.relu(x, name='Relu1')\n",
        "\n",
        "  x = tf.layers.conv2d(inputs=x, name='Conv2D2', padding='same',filters=64, kernel_size=[3,3], activation=None) \n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm2')\n",
        "  x = tf.nn.relu(x, name='Relu2')\n",
        "\n",
        "  x = tf.layers.max_pooling2d(inputs=x, pool_size=[2,2], strides=2)\n",
        "\n",
        "  x = tf.layers.conv2d(inputs=x, name='Conv2D3', padding='same',filters=64, kernel_size=[3,3], activation=None)\n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm3')\n",
        "  x = tf.nn.relu(x, name='Relu3')\n",
        "\n",
        "  x = tf.layers.conv2d(inputs=x, name='Conv2D4', padding='same',filters=64, kernel_size=[3,3], activation=None)\n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm4')\n",
        "  x = tf.nn.relu(x, name='Relu4')\n",
        "\n",
        "  x = tf.layers.max_pooling2d(inputs=x, pool_size=[2,2], strides=2)\n",
        "\n",
        "  x = tf.layers.conv2d(inputs=x, name='Conv2D5', padding='same',filters=128, kernel_size=[3,3], activation=None)\n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm5')\n",
        "  x = tf.nn.relu(x, name='Relu5')\n",
        "\n",
        "  x = tf.layers.conv2d(inputs=x, name='Conv2D6', padding='same',filters=128, kernel_size=[3,3], activation=None)\n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm6')\n",
        "  x = tf.nn.relu(x, name='Relu6')\n",
        "\n",
        "  x = tf.layers.max_pooling2d(inputs=x, pool_size=[2,2], strides=2)\n",
        "\n",
        "  x = tf.layers.conv2d(inputs=x, name='Conv2D7', padding='same',filters=128, kernel_size=[3,3], activation=None)\n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm7')\n",
        "  x = tf.nn.relu(x, name='Relu7')\n",
        "\n",
        "  x = tf.layers.conv2d(inputs=x, name='Conv2D8', padding='same',filters=128, kernel_size=[3,3], activation=None)\n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm8')\n",
        "  x = tf.nn.relu(x, name='Relu8')\n",
        "\n",
        "  #flattening layer\n",
        "  x = tf.contrib.layers.flatten(x)\n",
        "\n",
        "  #Fully-connected layers\n",
        "  x = tf.layers.dense(inputs=x, name='FC1',units=1024, activation=tf.nn.relu)\n",
        "  x = tf.layers.dropout(x,rate=0.5, training=True,name='Dropout')\n",
        "  x = tf.layers.batch_normalization(x, name='BatchNorm9')\n",
        "  H4 = tf.layers.dense(inputs=x, name='FCfinal',units=8, activation=None)\n",
        "\n",
        "  return H4"
      ],
      "metadata": {
        "id": "fbCjyFP3r_W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GenerateBatch(test_files_pa, test_files_pb, TrainLabels, MiniBatchSize):\n",
        "\n",
        "  patches_ab = []\n",
        "  labels = []\n",
        "\n",
        "  for i in range(MiniBatchSize):\n",
        "    index = random.randint(0, len(test_files_pa)-1) \n",
        "    patch_a = cv2.imread(test_files_pa[i], cv2.IMREAD_GRAYSCALE)\n",
        "    patch_b = cv2.imread(test_files_pb[i], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    patch_a = np.float32(patch_a)\n",
        "    patch_b = np.float32(patch_b)  \n",
        "\n",
        "    #combine images along depth\n",
        "    patch_ab = np.dstack((patch_a, patch_b))     \n",
        "    labels.append(TrainLabels[i,:].reshape(8,))\n",
        "    patches_ab.append(patch_ab)\n",
        "\n",
        "  return np.array(patches_ab), np.array(labels)\n",
        "\n",
        "def Test_sup(ImgPH,ModelPath, test_files_PA, test_files_PB, train_labels, image_files, SavePath):\n",
        "\n",
        "    if(not (os.path.isdir(SavePath))):\n",
        "        print(SavePath, \"  was not present, creating the folder...\")\n",
        "        os.makedirs(SavePath)\n",
        "    imgData,labels = GenerateBatch(test_files_PA, test_files_PB, train_labels, len(test_files_PA))\n",
        "\n",
        "    H4pt = homography_model(ImgPH, [128,128,2], len(test_files_PA))\n",
        "    Saver = tf.train.Saver()\n",
        "  \n",
        "    with tf.Session() as sess:\n",
        "        Saver.restore(sess, ModelPath)\n",
        "        print('Number of parameters in this model are %d ' % np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n",
        "        \n",
        "    \n",
        "        imgData=np.array(imgData).reshape(len(test_files_PA),128,128,2)\n",
        "        \n",
        "        FeedDict = {ImgPH: imgData}\n",
        "        Predicted = sess.run(H4pt,FeedDict)\n",
        "        h4pt_pred = np.array(Predicted)\n",
        "        print(h4pt_pred)\n",
        "        print(h4pt_pred.shape)\n",
        "        \n",
        "    np.save(SavePath+\"/h4pt_Pred.npy\",h4pt_pred)"
      ],
      "metadata": {
        "id": "Y1P1Wwp2rPdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    tf.reset_default_graph()\n",
        "    # base_dir = os.getcwd()\n",
        "    base_path ='/content/drive/MyDrive/Project_1' \n",
        "    test_files_PA = glob.glob( base_path + '/S_val/P_A'+ '/*.jpg')\n",
        "    test_files_PB = glob.glob( base_path + '/S_val/P_B'+ '/*.jpg')\n",
        "    image_files = glob.glob( base_path + '/S_val/I_A'+ '/*.jpg')\n",
        "    NumTestSamples = len(test_files_PA)\n",
        "    \n",
        "    ModelPath='/content/drive/MyDrive/Project_1/supervised_checkpoints/19model.ckpt'\n",
        "    SavePath = '/content/drive/MyDrive/Project_1/supervised_test'\n",
        "    train_labels = pd.read_csv(base_path + '/S_val/H_4pt.csv', index_col = False)\n",
        "    train_labels = train_labels.to_numpy()\n",
        "    ImgPH=tf.placeholder(tf.float32, shape=(len(test_files_PA), 128, 128, 2))\n",
        "    Test_sup(ImgPH,ModelPath, test_files_PA, test_files_PB, train_labels, image_files, SavePath)\n",
        "    pointsList = np.load(base_path + '/S_val/points_list.npy')\n",
        "    rand_i = np.random.randint(0,len(test_files_PA)-1, size=5)\n",
        "    for i in rand_i:\n",
        "        image = image_files[i]\n",
        "        comparison = Visualise_supervised(i, base_path + '/S_val', SavePath, train_labels, pointsList, image)\n",
        "        cv2.imwrite(SavePath+'/comparison'+ str(i)+'.png',comparison)\n",
        "    \n",
        "    print('Check Results/supervised folder..')     \n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lahTuQ3jrtKp",
        "outputId": "77d293b8-f69d-48be-f828-19efd8290958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Project_1/supervised_checkpoints/19model.ckpt\n",
            "Number of parameters in this model are 34195848 \n",
            "[[-9.010777  -7.0060797 10.01232   ... -4.0031695  1.0021203 15.026427 ]\n",
            " [-9.0103855 -7.0056806 10.011926  ... -4.0033216  1.002371  15.026286 ]\n",
            " [-9.011533  -7.0067954 10.013615  ... -4.003926   1.0022521 15.029782 ]\n",
            " ...\n",
            " [-9.008837  -7.005301  10.01055   ... -4.0029945  1.0022489 15.021788 ]\n",
            " [-9.010071  -7.005786  10.011204  ... -4.0032506  1.0022935 15.025472 ]\n",
            " [-9.008175  -7.0052524 10.009947  ... -4.0031157  1.0022079 15.022487 ]]\n",
            "(1000, 8)\n",
            "(1000, 8)\n",
            "Mean absolute Error for image at index  113 :   13.764420886640437\n",
            "(1000, 8)\n",
            "Mean absolute Error for image at index  394 :   20.26564726990182\n",
            "(1000, 8)\n",
            "Mean absolute Error for image at index  681 :   21.508347322349437\n",
            "(1000, 8)\n",
            "Mean absolute Error for image at index  509 :   19.014017799752764\n",
            "(1000, 8)\n",
            "Mean absolute Error for image at index  639 :   18.263399141957052\n",
            "Check Results/supervised folder..\n"
          ]
        }
      ]
    }
  ]
}